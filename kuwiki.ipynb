{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Wikipedia's n-grams base on the dump data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "The_path=r\"C:/Users/Reza/Desktop/core\"\n",
    "Dump_URL =u'https://dumps.wikimedia.org/kuwiki/latest/kuwiki-latest-pages-articles.xml.bz2'\n",
    "# download last dump\n",
    "os.system('wget '+Dump_URL +\" \"+The_path+\"/kuwiki-latest-pages-articles.xml.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code extracts features from articles:\n",
    "* Article text file\n",
    "* Category of the article\n",
    "* Internal links inside the articles\n",
    "* File and images in articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys , re\n",
    "# Load pywikibot library\n",
    "sys.path.append(The_path+'/pywikibot')\n",
    "sys.path.append(The_path)\n",
    "import xmlreader\n",
    "#import core\n",
    "#from pywikibot import xmlreader\n",
    "dump = xmlreader.XmlDump(The_path+r\"/kuwiki-latest-pages-articles.xml.bz2\")\n",
    "\n",
    "farsi_char=r'abcçdeêfghiîjklmnopqrsştuûvwxyzABCÇDEÊFGHIÎJKLMNOPQRSŞTUÛVWXYZ'# Persian Char + ZWNJ\n",
    "suported_extenstions=r'(?:[gt]iff?|png|jpe?g|web[mp]|xcf|pdf|mid|og[avg]|svg|djvu|flac|opus|wav|mp3)'\n",
    "\n",
    "def flat_text (text):\n",
    "    # clean and remove wiki sintax\n",
    "    text=text.replace('\\r','')\n",
    "    text=re.sub(r'\\{\\{[^\\}]+\\}\\}',' ',text)# remove wiki templates\n",
    "    text=re.sub('\\[\\[(?:Kategorî|Wêne)\\:.*?\\]\\]','',text) # remove categories and images\n",
    "    text=re.sub(r'\\<[^\\>]+\\>',' ',text) # remove commented texts\n",
    "    text=re.sub(r'\\n={1,}[^\\n]+\\={1,}',' ',text) # remove subtitles\n",
    "    text=re.sub(r'\\|[^\\=\\]\\n]+\\=',' ',text) # remove remained template variables\n",
    "    text=re.sub(r'\\[\\[[^\\|\\]]+\\|','',text) # to have correct text. I remove the piped text. eg. This is a [[foo|bar]] > This is bar\n",
    "    text=re.sub(r'\\.'+suported_extenstions,'',text)# remove useless . \n",
    "    text=text.replace('&ndash;',' ')# Replaced html space to not connect words\n",
    "    text=text.replace('[[',' ').replace(']]','')# replaced ]] and [[ with empty to connect the connected char after them. eg. [[book]]s > books\n",
    "    \n",
    "    text=re.sub(r'[^'+farsi_char+'\\.]+',' ',text)# removed none persian characters except . to find sentences\n",
    "    text=re.sub(r'\\.{1,}','.',text)# remove repeated .\n",
    "    text=re.sub(r'(\\. ){1,}','. ',text)# remove repeated .\n",
    "    \n",
    "    text=re.sub(r'\\s+',' ',text)# Replace \\n and space with one space\n",
    "    text=text.replace(' . ','. ')\n",
    "    return text\n",
    "\n",
    "category_dict={}\n",
    "links_dict={}\n",
    "image_dict={}\n",
    "title_dict={}\n",
    "text_dict={}\n",
    "\n",
    "counter=0\n",
    "for entry in dump.parse():\n",
    "    if entry.ns =='0' and not entry.isredirect:\n",
    "        counter+=1\n",
    "        text_dict[entry.id]=flat_text(entry.text)\n",
    "        title_dict[entry.id]=entry.title\n",
    "        #image_dict[entry.id]=get_images(entry.text)\n",
    "        #links_dict[entry.id]=get_links(entry.text)\n",
    "        #category_dict[entry.id]=get_categories(entry.text)\n",
    "        if counter % 50000==0:\n",
    "            print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp\n",
    "config = {\n",
    "    #'processors': 'tokenize,mwt,pos,lemma,depparse', # Comma-separated list of processors to use\n",
    "    'processors': 'tokenize,pos',\n",
    "    #'lang': 'fa', # Language code for the language to build the Pipeline in\n",
    "    'lang': 'kmr', # Language code for the language to build the Pipeline in\n",
    "}\n",
    "pipeline = stanfordnlp.Pipeline(**config)#lang='fa', use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\Reza\\Desktop\\ku\\ku_text_dict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text=f.read()\n",
    "len (text)\n",
    "for i in range(0,2000000,50000):\n",
    "    try:\n",
    "        doc = pipeline(text[i:50000+i])\n",
    "        with open(r\"C:\\Users\\Reza\\Desktop\\کردی\\ku_tokens_2.txt\",\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(u'\\n'+doc.conll_file.conll_as_string())\n",
    "    except:\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
